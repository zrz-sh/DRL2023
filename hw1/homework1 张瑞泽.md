# Homework 1

张瑞泽 2019011189 无98

## 2 Dynamic Programming

（c）FrozenLake-v1环境中，由于没有随机性，因此policy iteration和value iteration训练得到相同的策略，达到目标点，且成功率为100%，也就是绕过lake，挑最短距离到达目标点，且由于多个相同大小的值argmax默认取第一个，所以最短路径偏好性从大到小是：左-下-右-上。而对于SlipperyFrozenLake-v1环境来说，存在随机性，导致有几率无法在100步达到目标点，或者掉入lake。为了不掉入lake中，从起点出发，小人会选择向左走（为了往下走，且不往右走掉到湖中）。如果转移矩阵，变成面向某个方向是，只能前后滑动，不能侧边滑动，即向右走，有0.75概率向右走，0.25概率向左走（即向后摔跤），此时最优策略也会不同。

## 3 Temporal Difference Learning

（b）在0-1范围内修改了几次alpha和gamma，两种算法都能解决FrozenLake-v1环境的问题。看上去是这个问题难度较小，对于学习率和utility随step递减系数的敏感性不高。

（c）例如清华学子小A

状态空间有3个状态：学习，玩，睡觉。

动作空间有2个动作：奋斗，偷懒。

转移规律是：在任何状态，选择奋斗，就会进入学习状态（奖励为10）；选择偷懒，有1/3概率进入睡觉状态（奖励为5），有2/3概率进入玩的状态（奖励为0）。

由于是一个没有done的环境，可以采用截断进行处理，例如100步进行截断，得到一个trajectory。

如果假设白盒模型，就是概率已知，可以采用policy iteration或者value iteration进行求解得到最优策略。

如果假设黑盒模型，可以采用Q-learning或者SARSA通过与环境交互进行求解。